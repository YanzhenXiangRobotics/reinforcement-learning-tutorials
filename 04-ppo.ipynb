{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "recent-bunch",
   "metadata": {},
   "source": [
    "# PPO (Proximal Policy Optimization)\n",
    "\n",
    "Proximal Policy Optimization (PPO) is one of most successful algorithms in today's reinforcement learning.\n",
    "\n",
    "In order to avoid the loss of performance, PPO algorithm prevents the update from stepping so far.<br>\n",
    "In PPO, there are two variants: PPO-Penalty and PPO-Clip. In this example, we'll focus on PPO-Penalty, which is used in RLlib library.\n",
    "\n",
    "> Note : The idea of PPO-Clip is more simple rather than PPO-Penalty. PPO-Clip limits the update by directly clipping with $ \\epsilon $.<br>\n",
    "> For details about PPO-Clip, see [OpenAI document](https://spinningup.openai.com/en/latest/algorithms/ppo.html).\n",
    "\n",
    "As you saw in [Actor-Critic method](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/03-actor-critic.ipynb), the algorithm will learn policy parameters ($ \\theta $) with advantages $ A $.<br>\n",
    "When we assume that the agent takes large advantage $ A $ on action $ a $, $ P(a | \\pi_\\theta (s)) $ must be increased much more than $ P(a | \\pi_{\\theta_{old}} (s)) $, with new parameters $ \\theta $. Hence you can expect new $ \\theta $ as follows. :\n",
    "\n",
    "$$ \\max_{\\theta} E \\left[ \\frac{P(a | \\pi_\\theta (s))}{P(a | \\pi_{\\theta_{old}} (s))} A \\right] $$\n",
    "\n",
    "In order to prevent large policy updates, PPO penaltize for this expectation as follows :\n",
    "\n",
    "$$ \\max_{\\theta} E \\left[ \\frac{P(a | \\pi_\\theta (s))}{P(a | \\pi_{\\theta_{old}} (s))} A - \\beta \\cdot \\verb|penalty| \\right] $$\n",
    "\n",
    "where $ \\beta $ is the coefficient for the weight of penalty.\n",
    "\n",
    "In PPO-Penalty, KL-divergence is used as this penalty. :\n",
    "\n",
    "$$ \\verb|penalty| = \\verb|KL| \\left( P(\\cdot | \\pi_{\\theta_{old}} (s)) \\| P(\\cdot | \\pi_\\theta (s)) \\right) $$\n",
    "\n",
    "KL-divergence or Kullback-Leibler divergence is often used in information theory, such as, approximate inference.<br>\n",
    "Now we assume that both $ P(x) $ and $ Q(x) $ are stochastic distributions. Then $ \\verb|KL|( P \\| Q ) := -\\int{P(x) \\ln{\\frac{Q(x)}{P(X)}}}dx $ is always positive or zero, and zero if and only if both distributions are same.<br>\n",
    "$ \\verb|KL|( P \\| Q ) $ indicates how far between these distributions, $ P $ and $ Q $. If $ Q $ is so far from $ P $, $ \\verb|KL|( P \\| Q ) $ will be largely positive.\n",
    "\n",
    "> Note : For details about entropy and KL-divergence, see chapter 1.6 in \"[Pattern Recognition and Machine Learning](http://wordpress.redirectingat.com/?id=725X1342&isjs=1&jv=15.1.0-stackpath&sref=https%3A%2F%2Ftsmatz.wordpress.com%2F2020%2F06%2F01%2Fsvm-and-kernel-functions-mathematics%2F&url=https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fresearch%2Fuploads%2Fprod%2F2006%2F01%2FBishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&xguid=&xs=1&xtz=-540&xuuid=c861da822b99f831a421716ca3a51d33&xcust=8982&xjsf=other_click__auxclick%20%5B2%5D)\" (Christopher M. Bishop, Microsoft).\n",
    "\n",
    "In order to penaltize for large update between $ P(\\cdot | \\pi_{\\theta_{old}} (s)) $ and $ P(\\cdot | \\pi_\\theta (s)) $, we look for the optimal parameters $ \\theta $, such as :\n",
    "\n",
    "$$ \\max_{\\theta} E \\left[ \\frac{P(a | \\pi_\\theta (s))}{P(a | \\pi_{\\theta_{old}} (s))} A - \\beta \\cdot \\verb|KL| \\left( P(\\cdot | \\pi_{\\theta_{old}} (s)) \\| P(\\cdot | \\pi_\\theta (s)) \\right) \\right] $$\n",
    "\n",
    "Even if $ \\frac{P(a | \\pi_\\theta (s))}{P(a | \\pi_{\\theta_{old}} (s))} A $ largely increases, new $ \\theta $ might be rejected when the difference between $ P(\\cdot | \\pi_{\\theta_{old}} (s)) $ and $ P(\\cdot | \\pi_\\theta (s)) $ is so large.\n",
    "\n",
    "See [this paper](https://arxiv.org/pdf/1707.06347.pdf) for more details about PPO.\n",
    "\n",
    "> Note : In this example, we ignore (don't consider) an entropy regularizer.\n",
    "\n",
    "*(back to [index](https://github.com/tsmatz/reinforcement-learning-tutorials/))*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-cement",
   "metadata": {},
   "source": [
    "First, please install the required packages and import these modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy gym keras tensorflow==2.4 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "scenic-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-chaos",
   "metadata": {},
   "source": [
    "As you saw in [Actor-Critic](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/03-actor-critic.ipynb), now we build policy network $ \\pi(\\cdot) $ for the outputs of both action distribution (logits for categorical action) and value estimation. (i.e, this is a network for both policy function and value function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "entertaining-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_input = tf.keras.layers.Input(shape=(4, ))\n",
    "\n",
    "a_hidden = tf.keras.layers.Dense(16,activation=\"relu\")(s_input)\n",
    "a_output = tf.keras.layers.Dense(2,activation=\"relu\")(a_hidden)\n",
    "\n",
    "v_hidden = tf.keras.layers.Dense(16,activation=\"relu\")(s_input)\n",
    "v_output = tf.keras.layers.Dense(1,activation=None)(v_hidden)\n",
    "\n",
    "policy_pi = tf.keras.Model([s_input], [a_output, v_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-completion",
   "metadata": {},
   "source": [
    "Now we optimize both expectation (see above) and loss.\n",
    "\n",
    "Instead of maximizing $ \\frac{P(a | \\theta_{new})}{P(a | \\theta_{old})} A - \\beta \\cdot \\verb|KL| \\left( P(\\theta_{old}) \\| P(\\theta_{new}) \\right) $ and minimizing loss, here we minimize the following tensor :\n",
    "\n",
    "$$ (-1) \\frac{P(a | \\theta_{new})}{P(a | \\theta_{old})} A + \\beta \\cdot \\verb|KL| \\left( P(\\theta_{old}) \\| P(\\theta_{new}) \\right) + loss $$\n",
    "\n",
    "Unlike [Actor-Critic sample](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/03-actor-critic.ipynb), here I train the parameters with batch in each episode.\n",
    "\n",
    "> Note : The log probability equals to the negative value of cross-entropy error. Same like [policy gradient example](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/02-policy-gradient.ipynb), I used ```-tf.nn.sparse_softmax_cross_entropy_with_logits()``` to get log probability in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sublime-basin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run episode2999 with rewards 200.0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99    # discount\n",
    "kl_coeff = 0.2  # weight coefficient for KL-divergence loss\n",
    "vl_coeff = 1.0  # weight coefficient for value loss\n",
    "\n",
    "# pick up action and following properties for state (s)\n",
    "# Return :\n",
    "#     action (int)       action\n",
    "#     logits (list[int]) logits defining categorical distribution\n",
    "#     logprb (float)     log probability\n",
    "def pick_sample_and_logp(s):\n",
    "    logits_t, _ = policy_pi(tf.convert_to_tensor(np.array([s])))\n",
    "    # return tf.Tensor([[a]], shape=(1, 1), dtype=int64)\n",
    "    action_t = tf.random.categorical(logits_t, 1)\n",
    "    # return tf.Tensor([a], shape=(1, 1), dtype=int64)\n",
    "    action_t = tf.squeeze(action_t, axis=1)\n",
    "    # calculate log probability\n",
    "    logprb_t = -tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_t,\n",
    "        labels=action_t)\n",
    "    # get numpy action\n",
    "    action = int(action_t.numpy()[0])\n",
    "    # get numpy logits\n",
    "    logits = logits_t.numpy()[0]\n",
    "    # get numpy logprb\n",
    "    logprb = logprb_t.numpy()[0]\n",
    "    return action, logits, logprb\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "reward_records = []\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "for i in range(3000):\n",
    "    # Run episode till done\n",
    "    done = False\n",
    "    states_arr = []\n",
    "    action_arr = []\n",
    "    logits_arr = []\n",
    "    logprb_arr = []\n",
    "    reward_arr = []\n",
    "    s = env.reset()\n",
    "    while not done:\n",
    "        states_arr.append(s)\n",
    "        a, p, l = pick_sample_and_logp(s)\n",
    "        s_next, r, done, _ = env.step(a)\n",
    "        action_arr.append([a])\n",
    "        logits_arr.append(p)\n",
    "        logprb_arr.append([l])\n",
    "        reward_arr.append([r])\n",
    "        s = s_next\n",
    "\n",
    "    # Change to cumulative rewards\n",
    "    rewcum_arr = np.zeros_like(reward_arr)\n",
    "    reward_len = len(reward_arr)\n",
    "    for j in reversed(range(reward_len)):\n",
    "        cum_reward = reward_arr[j][0] + (rewcum_arr[j+1][0]*gamma if j+1 < reward_len else 0)\n",
    "        rewcum_arr[j] = [cum_reward]\n",
    "\n",
    "    # Output total rewards in episode (max 200)\n",
    "    print(\"Run episode{} with rewards {}\".format(i, np.sum(reward_arr)), end=\"\\r\")\n",
    "    reward_records.append(np.sum(reward_arr))\n",
    "\n",
    "    # Train parameters\n",
    "    with tf.GradientTape() as tape:\n",
    "        states_arr_t = tf.convert_to_tensor(states_arr)\n",
    "        action_arr_t = tf.convert_to_tensor(action_arr)\n",
    "        logits_arr_t = tf.convert_to_tensor(logits_arr)\n",
    "        logprb_arr_t = tf.convert_to_tensor(logprb_arr)\n",
    "        rewcum_arr_t = tf.convert_to_tensor(rewcum_arr, dtype=tf.float32)\n",
    "        logits_new_t, value_new_t = policy_pi(states_arr_t, training=True)\n",
    "\n",
    "        # Get advantage\n",
    "        advantage_t = rewcum_arr_t - value_new_t\n",
    "\n",
    "        # Calculate P_new / P_old\n",
    "        logprb_new_t = -tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits_new_t,\n",
    "            labels=tf.squeeze(action_arr_t, axis=1))\n",
    "        logprb_new_t = tf.expand_dims(logprb_new_t, 1)\n",
    "        prob_ratio = tf.exp(logprb_new_t - logprb_arr_t)\n",
    "\n",
    "        # Calculate KL-div for Categorical distribution\n",
    "        a0 = logits_arr_t - tf.reduce_max(logits_arr_t, axis=1, keepdims=True)\n",
    "        a1 = logits_new_t - tf.reduce_max(logits_new_t, axis=1, keepdims=True)\n",
    "        ea0 = tf.exp(a0)\n",
    "        ea1 = tf.exp(a1)\n",
    "        z0 = tf.reduce_sum(ea0, axis=1, keepdims=True)\n",
    "        z1 = tf.reduce_sum(ea1, axis=1, keepdims=True)\n",
    "        p0 = ea0 / z0\n",
    "        kl = tf.reduce_sum(\n",
    "            p0 * (a0 - tf.math.log(z0) - a1 + tf.math.log(z1)), axis=1)\n",
    "        kl = tf.expand_dims(kl, 1)\n",
    "\n",
    "        # Get value loss\n",
    "        v_loss = tf.math.square(value_new_t - rewcum_arr_t)\n",
    "\n",
    "        # Minimize loss\n",
    "        grad = tape.gradient(\n",
    "            -advantage_t * prob_ratio + kl_coeff * kl + vl_coeff * v_loss,\n",
    "            policy_pi.trainable_variables)\n",
    "        opt.apply_gradients(zip(grad, policy_pi.trainable_variables))\n",
    "\n",
    "print(\"\\nDone\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "every-appointment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f43399facc0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Generate recent 100 interval average\n",
    "reward_records_np = np.array(reward_records)\n",
    "average_reward = []\n",
    "for idx in range(len(reward_records)):\n",
    "    avg_list = np.empty(shape=(1,), dtype=int)\n",
    "    if idx < 100:\n",
    "        avg_list = reward_records_np[:idx+1]\n",
    "    else:\n",
    "        avg_list = reward_records_np[idx-99:idx+1]\n",
    "    average_reward.append(np.average(avg_list))\n",
    "plt.plot(average_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33556ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
